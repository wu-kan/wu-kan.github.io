---
redirect_from: /_posts/2020-11-25-%E8%B6%85%E7%AE%97%E9%98%9F%E5%86%85%E5%9F%B9-HPL%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93/
layout: home
title: 超算队内培：HPL作业总结
tags:
  - 高性能计算
---

## 跑分任务总结

<!-- .slide vertical=true -->

|        (/TFLOPS)         | Theoretical value | Proposal | Yours                  |
| :----------------------: | :---------------: | :------: | ---------------------- |
|    HPL(4$\times$cpn)     |       12.4        |   8.45   | 8.76（姜智瀚）         |
| HPL(2$\times$4 张 v100)  |       62.4        |  33.89   | 36.19（冯浚轩）        |
|    HPCG(4$\times$cpn)    |       12.4        |   0.06   | 0.13（张景润，姜智瀚） |
| HPCG(2$\times$4 张 v100) |       62.4        |   1.08   | 1.07（冯浚轩）         |

<!-- .slide -->

### 计算理论性能

<!-- .slide vertical=true -->

- [Intel Xeon Gold 6150](https://ark.intel.com/content/www/cn/zh/ark/products/120490/intel-xeon-gold-6150-processor-24-75m-cache-2-70-ghz.html)
- 18 核 36 线程，TDP 165W
- 单核睿频 3.7 GHz，全核睿频 3.4Ghz，AVX2 全核睿频 3Ghz
- AVX-512 只能运行在 2.7Ghz 以下，有 2 个 AVX-512 FMA 单元
- [每时钟周期浮点运算数理论值](http://blog.sysu.tech/Benchmark/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97CPU%E7%AE%97%E5%8A%9B%E7%90%86%E8%AE%BA%E5%B3%B0%E5%80%BC/) $\frac{512}{64}\times 4=32$
- HPL 是计算密集型程序，应当关闭超线程！

<!-- .slide vertical=true -->

```bash
[cpn233]$ cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c
     36  Intel(R) Xeon(R) Gold 6150 CPU @ 2.70GHz
```

- $4\text{Nodes}\times 36\text{Cores}\times 2.7 \text{GHz}\times32\text{IPC}=12441.6 \text{GFLOPS}$
- 节点数 x 单节点物理核数 x 单核运行频率 x 每时钟周期浮点运算数

<!-- .slide vertical=true -->

```bash
[gpu55]$ nvidia-smi --query-gpu=clocks.max.sm --format=csv --id=0
clocks.max.sm [MHz]
1530 MHz
```

- V100 SXM2 单卡有 5120 个 CUDA 核心，2560 个 FP64 单元。
- 双精度性能为 $1530\text{MHz}\times2560\text{Cores}\times 2\approx 7.8\text{TFLOPS}$
- ASC19 现场用的 V100 PCI-E 稍弱一些，$7 \text{TFLOPS}$
- 也可在[官网](https://www.nvidia.cn/data-center/v100/)上直接查到性能数据
- 明年比赛大概率会使用 [A100 PCI-E](https://www.nvidia.cn/data-center/a100/)
  - 安培架构的 TensorCore 可做 FP64 矩阵乘法，要考虑进去！

<!-- .slide -->

### 选择针对硬件架构特别优化的软件软件包

<!-- .slide vertical=true -->

- 反例：轩轩一开始使用英伟达官网上八年前的 hpl-2.0_FERMI_v15 进行跑分！
  - 直接调用 cuBLAS，未对规模小的矩阵优化
  - 只能跑到理论性能的十分之一
- 明年 A100 是新的 Ampere 架构，不能使用之前的二进制包（只支持到 Volta）！
  - 单卡性能 19.5TFLOPS（TensorCore）
- CPU 上的跑分，一般使用 MKL 中提供的即可。

<!-- .slide -->

### 优化通信

<!-- .slide vertical=true -->

> （使用两个节点后）…现在效率只有 58%了，怎么看这个数值都太感人了吧，可能是其它的小参数有限制，也可能是网络原因

- 引入多节点进行计算后会由于节点间延迟高通信的原因导致性能下降

<!-- .slide vertical=true -->

```bash
export I_MPI_FAVRICS=shm:dapl # 节点内共享内存，节点间用dapl
```

- 使用 IntelMPI 的时候可以配置 [InfiniBand](http://blog.sysu.tech/MPI/Intel%20MPI/I_MPI_FABRICS/) 来优化连接带宽。
- Intel MPI 提供了名为 Direct Access Programming Library（DAPL）的中间层来支持多架构，兼容多种网络硬件及协议，优化网络互联。
- 其他 MPI 实现也有类似手段。可以参照去年永锋的 wiki：重新编译 openmpi + 不用 tcp 多机 GPU HPL。
- 总之，在网卡、交换机支持的情况下，尽量少用 TCP！

<!-- .slide -->

### 调参

> Linpack 算法可以说是最精妙的并行算法，算法本身的细节可以通过大量调整参数，应用于各种不同计算环境的 Benchmark（杜总）

- 不同于机器学习中的“炼丹玄学”
- 讲武德，遵守基本法（则）

<!-- .slide -->

#### 第 5\~6 行 Ns 问题规模

<!-- .slide vertical=true -->

- Amdahl 定律
  - $\text{Speedup}=1/\left(\left(1-f\right)+\frac{f}{m}\right)$
  - $f$ 为问题的并行化比例，$m$ 为并行核数
  - 问题规模 N 固定时，可并行化的比例是固定的，加速比有上限

<!-- .slide vertical=true -->

- Gustafson 定律
  - $\text{Speedup}=(1-f)+mf$
  - 问题规模不固定时，问题并行化程度越高，加速比越接近于并行核数
  - 对 Amdahl 模型的补充修正，重拾对大规模并行计算的信心

<!-- .slide vertical=true -->

- 一般问题规模越大越好
  - 不造成内存页交换
  - 系统总内存（显存）的 80%?
  - 系统空闲内存（显存）的 90%?

<!-- .slide -->

#### 第 7\~8 行 NBs 分块大小（计算粒度）

<!-- .slide vertical=true -->

- 矩阵被分成 $\text{NB}\times\text{NB}$ 的循环块，分配到各个进程当中去处理
- 取决于单进程所能调度的资源限制
  - CPU 核数
  - CPU 向量化指令同时操作的元素数
  - CPU 对应的缓存大小
  - GPU 单线程束的宽度
  - 单进程占用内存（显存）

<!-- .slide vertical=true -->

- 一般 N 要微调成 NB 的整数倍，防止边缘处性能下降
- $\text{NB}\times 8$ 一定是 Cache line 的倍数
- 对于 AVX-512 指令集优化的 benchmark，一般来说是 384
  - 实际上还是都测一下
- 对于 GPU 上的 benchmark，一般至少要设置成 32 的整数倍
  - 一个 Warp 的宽度
- NB 过大容易导致 Cache Miss

<!-- .slide -->

#### 第 9 行 PMAP process mapping 处理器阵列排布方式

<!-- .slide vertical=true -->

- 按行的排列方式适用于节点数较少、每个节点内 CPU 数较多的系统
- 按列的排列方式适用于节点数较多、每个节点内 CPU 数较少的系统
- 一般在大规模集群系统上，按列的排列方式的性能远好于按行的排列方式
- 小型集群上，行优先略优

<!-- .slide -->

#### 第 11\~12 行 P × Q 二维进程映射

<!-- .slide vertical=true -->

- P × Q = 进程数
  - 一般来说一个进程对于一个 CPU 可以得到最佳性能
- P 的值尽量取得小一点（$P\le Q$），因为向量维度相同时，列向通信量（通信次数和通信数据量）要远大于横向通信。
- P 不宜过小（1），不利于计算过程中通过 Lookahead 掩盖通信开销
- $P = 2^n$，即 P 最好选择 2 的幂。
  - HPL 中，L 分解的列向通信可选二元交换法（Binary Exchange）。

<!-- .slide -->

#### 第 13 行 threshold 检验结果时的计算精度

- 不许改，使用默认的 16.0
  - 不然结果非法就白跑了

<!-- .slide -->

#### 第 14\~21 行 递归分解的方式

<!-- .slide vertical=true -->

- 每次完成 NB 列的消元，然后更新后面的矩阵，NB 的消元就是 L 的分解
- 每次 L 的分解只在一列处理器中完成
- PFACTs 和 RFACTs 存在三种方法，对应参数的含义详见参考论文
  - Left-looking
  - Crout-looking
  - Right-looking
  - 对性能的影响不大，一般使用经验值（1 或 2）
- NBMINs,NDIVS 取一样的值，经验值 2

<!-- .slide -->

#### 第 23 行 BCASTs L 的横向广播方式

<!-- .slide vertical=true -->

- `0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM`
- 前 4 种适合于快速网络
- 后 2 种采用将数据切割后传送的方式，主要适合于速度较慢的网络
  - 一般不采用后两种方式。
- 一般来说
  - 在小规模系统中，选择 0 或 1
  - 对于大规模系统，选择 3

<!-- .slide -->

#### 第 25 行 DEPTHs L 的横向通信的通信深度

- 依赖于机器的配置和问题规模的大小
- 经验值 1（优先）,0

<!-- .slide -->

#### 第 26\~27 行 U 的广播算法

- U 的广播为列向广播，HPL 提供了 3 种 U 的广播算法：
  - 二元交换（Binary Exchange）法
  - Long 法
  - 混合法
- 推荐 Long 法

<!-- .slide -->

#### 第 28\~29 行 L 和 U 的数据存放格式

- 若选择“transposed”，则采用按列存放，否则按行存放。
- 影响 Cache Miss
- GPU 上还要考虑合并访存的问题
- 推荐第一个 1，第二个 0

<!-- .slide -->

#### 第 30 行 Equilibration

- 主要在回代中使用
- 对性能影响极小，可使结果更精确
- 建议打开

<!-- .slide -->

#### 第 31 行 memory alignment in double

- 用于在内存分配中对齐地址
- 在向量化指令中有更佳表现
- 一般设为 8 的整数倍

<!-- .slide -->

## PRESTO 赛题探索

- 甘家振
  - 在天河上部署所有依赖，并完成 modulefile
  - 在天河上成功运行两个算例
- 冯浚轩
  - 在天河上部署所有依赖（使用 spack）
- 吕天翔
  - 在自己实验室的集群上运行两个算例

<!-- .slide -->

## 下一阶段任务

- 自己有更多的想法可以自行探索
- 每个人的工作在 wiki 上展示出来的方式
  - 争取做到“一个人做了等于大家都做了”的效果
  - 切忌眼高手低
- 尽可能提高合作效率
  - 避免无谓的重复试错

<!-- .slide vertical=true -->

- 在天河上部署 PRESTO 环境依赖并成功跑通两个算例
  - 后续在天河上测已优化程序的可扩展性
  - 天河上已有安装好的 glib 和 fftw

<!-- .slide vertical=true -->

- 分析两个官方算例，搞清楚一下几个问题
  - 建议借助各种工具（advisor/vtune/aps，或自行探索其他工具）
  - 这两个算例在算什么，有哪些流程？
  - 每个步骤的耗时情况？
  - 代码的热点、瓶颈在哪里（计算/访存/通信）？
  - 各部分数据的依赖情况？
  - 哪些地方可以并行？
  - 整理一份初步的 proposal 草稿（中/英文）
  - 做一些数据可视化工作

<!-- .slide vertical=true -->

- 探索不同软件环境下的性能对比
  - 不同编译器
  - 不同编译选项
  - 不同 MPI
  - python/ipython
  - 最好能有一个自动化测试脚本，可以在决赛现场使用

<!-- .slide vertical=true -->

- 尝试使用 mkl fftw 或 cufft 代替 fftw
  - 一个可行方向，暂不清楚 fftw 在总时间里的占比
  - cufft 的接口和 fftw 接口略有差异，不过应该可以完成

<!-- .slide vertical=true -->

- 阅读[论文](https://wu-kan.github.io/sysu-thesis/main.pdf)中对 HPL 算法的详细介绍
- 阅读 [HPCG 3.0 reference implementation 阅读笔记](https://enigmahuang.me/2017/12/27/HPCG_3_Notes/)
  - 思考 HPCG 跑分时候如何分配进程到节点的映射
- 使用 3 机 12 卡，破 ASC19 HPL 跑分记录（50.21 TFlops）

<!-- .slide -->

## 下期内培预告

- 本周末，具体时间地点待定
- By 黄承欢：选拔赛优化组必做题讲解+如何舒服地进行调优工作
  - 优化至每轮迭代 1.4ms
